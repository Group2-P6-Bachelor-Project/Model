{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Group2-P6-Bachelor-Project/Model/blob/main/P6_Bachelor_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBf484kFGrpz"
      },
      "source": [
        "Mounting and Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYe-U-F8qaVR",
        "outputId": "f266eb83-5d87-4c4d-c2ac-3284db231547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyunpack\n",
            "  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n",
            "Collecting entrypoint2\n",
            "  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting easyprocess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: entrypoint2, easyprocess, pyunpack\n",
            "Successfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n",
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.12.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? "
          ]
        }
      ],
      "source": [
        "!pip install pyunpack\n",
        "!pip install patool\n",
        "!pip uninstall tensorflow\n",
        "!pip install tf-nightly\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OXBe4MLpB4d"
      },
      "outputs": [],
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "from pyunpack import Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEuCCER9ejz1"
      },
      "outputs": [],
      "source": [
        "!mkdir my_dataset\n",
        "Archive('/content/gdrive/MyDrive/InitialV2.rar').extractall('/content/my_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz-616sTj3bt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path('/content/my_dataset/InitialV2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MbDOsXn02K6"
      },
      "source": [
        "\n",
        "# Attempt #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OznGh3G-gv1"
      },
      "outputs": [],
      "source": [
        "img_size = (260, 260)\n",
        "batch_size = 32\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "seeds = [789]\n",
        "for seed in seeds:\n",
        "  train_ds, val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      data_dir,\n",
        "      validation_split=0.2,\n",
        "      subset= 'both',\n",
        "      seed=seed,\n",
        "      image_size=img_size,\n",
        "      batch_size=batch_size)\n",
        "\n",
        "  data_augmentation = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "      tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "      tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "  ])\n",
        "\n",
        "  train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
        "  label_names = sorted(item.name for item in data_dir.glob('*/') if item.is_dir())\n",
        "  label_encoder = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
        "  label_encoder.adapt(label_names)\n",
        "\n",
        "  # Map the labels to integers\n",
        "  train_labels = train_ds.map(lambda x, y: label_encoder(y))\n",
        "  val_labels = val_ds.map(lambda x, y: label_encoder(y))\n",
        "\n",
        "  num_classes = len(label_encoder.get_vocabulary())\n",
        "\n",
        "  base_model = tf.keras.applications.EfficientNetB2(include_top=False, weights='imagenet', input_shape=(260, 260, 3))\n",
        "  base_model.trainable = False\n",
        "  inputs = tf.keras.Input(shape=(260, 260, 3))\n",
        "  x = base_model(inputs, training=False)\n",
        "  #Andres said it should be removed but removing it breaks the code\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\\\n",
        "  #Add more layers her\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  #show model.summary\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(train_ds,\n",
        "                      validation_data=val_ds,\n",
        "                      epochs=10)\n",
        "  train_accs.append(history.history['accuracy'][-1])\n",
        "  val_accs.append(history.history['val_accuracy'][-1])\n",
        "\n",
        "for i in range(len(seeds)):\n",
        "    print(f'Seed {seeds[i]}: Train accuracy = {train_accs[i]}, Validation accuracy = {val_accs[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsD_0ty5auP_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "# Show the legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# Show the legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjniuUKw8tvf"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# Get the true labels and predicted labels\n",
        "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "y_pred = np.argmax(model.predict(val_ds), axis=-1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(val_ds.class_names))\n",
        "plt.xticks(tick_marks, val_ds.class_names, rotation=45)\n",
        "plt.yticks(tick_marks, val_ds.class_names)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-o9x6fPxrHh"
      },
      "outputs": [],
      "source": [
        "# def plot_misclassified_images(val_test_ds, y_true, y_pred):\n",
        "#     for i, (image, label) in enumerate(val_test_ds.unbatch()):\n",
        "#         if y_true[i] != y_pred[i]:\n",
        "#             plt.figure(figsize=(4,4))\n",
        "#             plt.imshow(image.numpy().astype(np.uint8))\n",
        "#             plt.title(f'True label: {val_test_ds.class_names[y_true[i]]}, Predicted label: {val_test_ds.class_names[y_pred[i]]}')\n",
        "#             plt.axis('off')\n",
        "#             plt.show()\n",
        "# plot_misclassified_images(val_ds, y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwCB9Z2RHD3L"
      },
      "outputs": [],
      "source": [
        "#Fine-Tuning add custom layers that fit our problem\n",
        "#Freeze the base layers of the model and train only the custom layers that we added\n",
        "#Unfreeze the top layers of the model and retrain them together with the custom layers\n",
        "#Compare the results you are getting and modify the layers to get better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByyQGmKF70gt"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import os\n",
        "# # Get a batch of images and labels\n",
        "# images, labels = next(iter(val_ds))\n",
        "# image = tf.keras.preprocessing.image.load_img('/content/IMG_0947.jpg', target_size=(260, 260))\n",
        "\n",
        "# image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
        "# image_array = tf.keras.applications.efficientnet.preprocess_input(image_array)\n",
        "# image_tensor = tf.convert_to_tensor(image_array)\n",
        "# image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
        "# # Make predictions on the batch of images\n",
        "# predictions = model.predict(image_tensor)\n",
        "\n",
        "# # Get the predicted classes\n",
        "# predicted_class = np.argmax(predictions, axis=1)[0]\n",
        "# predicted_probability = predictions[0][predicted_class]\n",
        "# # Get the true classes\n",
        "# class_names_copy = label_names.copy()\n",
        "# predicted_name = class_names_copy.pop(predicted_class)\n",
        "\n",
        "# # Get the accuracy\n",
        "# other_probabilities = [predictions[0][i] for i in range(len(class_names_copy))]\n",
        "\n",
        "# other_sum = sum(other_probabilities)\n",
        "\n",
        "# # Normalize the probabilities of the other classes\n",
        "# other_probabilities = [p / other_sum * (1 - predicted_probability) for p in other_probabilities]\n",
        "\n",
        "# # Display the image and the predicted class\n",
        "# img = images[0]\n",
        "# class_names = sorted(os.listdir(data_dir))\n",
        "\n",
        "# plt.imshow(image)\n",
        "# plt.title(f'Predicted: {class_names[predicted_class]}, Accuracy: {predicted_probability:.2f}')\n",
        "# plt.show()\n",
        "# # Display the other classes and their probabilities\n",
        "# for i in range(len(class_names_copy)):\n",
        "#     print(f'{class_names_copy[i]}: {other_probabilities[i]:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model2.h5\")"
      ],
      "metadata": {
        "id": "S0t2Qj-F-Q5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TttiqGlbkz4d"
      },
      "outputs": [],
      "source": [
        "# !zip -r /content/hundred_epoch_model.zip /content/model\n",
        "# from google.colab import files\n",
        "# files.download('/content/hundred_epoch_model.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yyewlvNldsJ"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# tflite_output_path = '/content/converted_model.tflite'\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# with open(tflite_output_path, 'wb') as f:\n",
        "#     f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je64FJGhWhOM"
      },
      "source": [
        "Testing tflite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFfvrXxqWmrE"
      },
      "outputs": [],
      "source": [
        "# interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
        "# interpreter.allocate_tensors()\n",
        "\n",
        "# input_details = interpreter.get_input_details()\n",
        "# output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCwQApB4WpHh"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# # Define the data augmentation pipeline\n",
        "# data_augmentation = tf.keras.Sequential([\n",
        "#   tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "#   tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "#   tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "# ])\n",
        "# image = tf.keras.preprocessing.image.load_img('/content/IMG_0947.jpg', target_size=(260, 260))\n",
        "\n",
        "# # Apply the data augmentation to the input image\n",
        "# # image = data_augmentation(np.array(image))\n",
        "\n",
        "\n",
        "# image = np.array(image, dtype=np.float32)\n",
        "\n",
        "\n",
        "# image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
        "# image = np.expand_dims(image, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oha8VjdpWquq"
      },
      "outputs": [],
      "source": [
        "# interpreter.set_tensor(input_details[0]['index'], image)\n",
        "# interpreter.invoke()\n",
        "# output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "# predicted_labels = np.argmax(output_data, axis=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHWNhqpiWv4A"
      },
      "outputs": [],
      "source": [
        "# # Display the image and the predicted labels\n",
        "# plt.imshow(image[0])\n",
        "# plt.title(\"Predicted label: {}\".format(class_names[predicted_labels]))\n",
        "# plt.show()\n",
        "\n",
        "# # Print the predicted probabilities for all labels\n",
        "# for i in range(len(class_names)):\n",
        "#     print(\"{}: {:.2f}%\".format(class_names[i], output_data[0][i] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hDLpuFTWxRS"
      },
      "outputs": [],
      "source": [
        "# print(image[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsGk_a7Fpph3"
      },
      "source": [
        "Validation with different datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JJHGLm6M4B5"
      },
      "outputs": [],
      "source": [
        "!mkdir my_validationset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCULlDe4psCR"
      },
      "outputs": [],
      "source": [
        "from pyunpack import Archive\n",
        "Archive('/content/gdrive/MyDrive/Validation_WastePicture.zip').extractall('/content/my_validationset2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMqhFnpJrjFi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pathlib\n",
        "val_dir = pathlib.Path('/content/my_validationset2/Validation_WastePicture')\n",
        "val_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      val_dir,\n",
        "      image_size=img_size,\n",
        "      validation_split=0,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uj4-AdfwIp3"
      },
      "outputs": [],
      "source": [
        "#Confusion Matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# Get the true labels and predicted labels\n",
        "y_test_true = np.concatenate([y for x, y in val_test_ds], axis=0)\n",
        "y_test_pred = np.argmax(model.predict(val_test_ds), axis=-1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(val_test_ds.class_names))\n",
        "plt.xticks(tick_marks, val_test_ds.class_names, rotation=45)\n",
        "plt.yticks(tick_marks, val_test_ds.class_names)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], 'd'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv0k7bzCxBxg"
      },
      "outputs": [],
      "source": [
        "# plot_misclassified_images(val_test_ds, y_test_true, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUaeF8wHTTl1"
      },
      "source": [
        "Testning 100 Epoch model with a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gEOWRl8TTQ1"
      },
      "outputs": [],
      "source": [
        "# Archive('/content/gdrive/MyDrive/hundred_epoch_model.zip').extractall('/content/')\n",
        "# saved_model_path = '/content/model2/'\n",
        "# # Load the saved model\n",
        "# loaded_model = tf.saved_model.load(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8lf41EmUbvh"
      },
      "outputs": [],
      "source": [
        "# #Confusion Matrix\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# import itertools\n",
        "\n",
        "# # Get the true labels and predicted labels\n",
        "# y_test_true = np.concatenate([y for x, y in val_test_ds], axis=0)\n",
        "# y_test_pred = np.argmax(loaded_model.predict(val_test_ds), axis=-1)\n",
        "\n",
        "# # Compute the confusion matrix\n",
        "# cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "\n",
        "# # Plot the confusion matrix\n",
        "# plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "# plt.title('Confusion Matrix')\n",
        "# plt.colorbar()\n",
        "# tick_marks = np.arange(len(val_test_ds.class_names))\n",
        "# plt.xticks(tick_marks, val_test_ds.class_names, rotation=45)\n",
        "# plt.yticks(tick_marks, val_test_ds.class_names)\n",
        "\n",
        "# thresh = cm.max() / 2.\n",
        "# for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "#     plt.text(j, i, format(cm[i, j], 'd'),\n",
        "#              horizontalalignment=\"center\",\n",
        "#              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.ylabel('True label')\n",
        "# plt.xlabel('Predicted label')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensembling"
      ],
      "metadata": {
        "id": "vg3KBv0LCMT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tf.keras.models.load_model('model.h5')\n",
        "model2 = tf.keras.models.load_model('model2.h5')\n",
        "model3 = tf.keras.models.load_model('100_epoch.h5')"
      ],
      "metadata": {
        "id": "4NBxzl4YCN7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def ensemble_predict(models, dataset):\n",
        "    predictions = []\n",
        "    for model in models:\n",
        "        predictions.append(model.predict(dataset))\n",
        "    avg_predictions = np.mean(predictions, axis=0)\n",
        "    return avg_predictions\n",
        "\n",
        "ensemble_predictions = ensemble_predict([model1, model2, model3], val_test_ds)"
      ],
      "metadata": {
        "id": "t7ZQW4HGCcqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_labels = np.concatenate([y for x, y in val_test_ds], axis=0)\n",
        "# convert ensemble predictions to class labels\n",
        "ensemble_labels = np.argmax(ensemble_predictions, axis=1)\n",
        "ground_truth_labels = ground_truth_labels.astype('float32')"
      ],
      "metadata": {
        "id": "TYlWBcCCCiVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ensemble_labels.shape, ground_truth_labels.shape)"
      ],
      "metadata": {
        "id": "jZQ4uu2WFeEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_metrics = model.evaluate(ensemble_labels, ground_truth_labels)"
      ],
      "metadata": {
        "id": "6IXAK3LOGDVw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}